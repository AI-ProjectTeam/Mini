"""
ìºë¦­í„° ìƒì„± AI ëª¨ë¸ ì„œë¹„ìŠ¤
ê³¤ì¶© ì´ë¯¸ì§€ë¥¼ ê·€ì—¬ìš´ ìºë¦­í„°ë¡œ ë³€í™˜í•˜ëŠ” ìƒì„±í˜• AI ëª¨ë¸ ê´€ë¦¬

ì£¼ìš” ê¸°ëŠ¥:
1. ê³¤ì¶© ì´ë¯¸ì§€ ë¶„ì„
2. ìºë¦­í„° ìŠ¤íƒ€ì¼ ì ìš©
3. ê·€ì—¬ìš´ ìºë¦­í„° ì´ë¯¸ì§€ ìƒì„±

íŒ€ì›ì´ ì‹¤ì œ ìƒì„± ëª¨ë¸ì„ êµ¬í˜„í•  ë•Œ ì´ í´ë˜ìŠ¤ë¥¼ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.
"""

import os
import asyncio
from PIL import Image, ImageDraw, ImageFont
import torch
import random
import logging
import io
import time
from fastapi import HTTPException
from typing import Dict, List, Tuple
from datetime import datetime
from diffusers import AutoPipelineForText2Image

# ë¡œê¹… ì„¤ì • - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•´
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CharacterGenerator:
    """
    ê³¤ì¶©ì„ ê·€ì—¬ìš´ ìºë¦­í„°ë¡œ ë³€í™˜í•˜ëŠ” ìƒì„±í˜• AI ëª¨ë¸ í´ë˜ìŠ¤
    í˜„ì¬ëŠ” ë”ë¯¸ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ë©°, ì‹¤ì œ ëª¨ë¸ êµ¬í˜„ ì‹œ ìˆ˜ì • í•„ìš”
    """
    
    def __init__(self):
        """
        ìºë¦­í„° ìƒì„± ëª¨ë¸ ì´ˆê¸°í™”
        ì‹¤ì œ ìƒì„± ëª¨ë¸(GAN, Diffusion ë“±) ë¡œë”©ì€ ì—¬ê¸°ì„œ ìˆ˜í–‰
        """

    #     self.prompt = (
    #     "cute chibi {keyword} insect, kawaii character, pure insect anatomy, round body, six legs, vibrant wings, big adorable eyes, cute antennae, simple clean art, children's book illustration cartoon, white background, happy, no anthropomorphism, high quality"
    # )
    #     self.negative_prompt = "poorly drawn, bad anatomy, deformed, ugly, extra limbs, incorrect number of legs, mutated, merged body parts, human, anthropomorphic, text, watermark, signature, blurred, grainy, realistic, 3d, complex background, dull colors, grayscale, multiple heads, too many eyes"
            
        self.prompt = (
            "cute cartoon {keyword}, insect body with six legs, "
            "colorful wings, antennae, chibi style{keyword}, kawaii{keyword}, bright cheerful colors, "
            "children's book illustration, simple clean art style, white background, "
            "NOT human, NOT anthropomorphic, pure insect anatomy, adorable bug character"
        )

        # ì•± ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ
        logger.info("ëª¨ë¸ ë¡œë”© ì‹œì‘...")
        model_load_start_time = time.time()

        try:
            # ì‹¤ì œ ìƒì„± ëª¨ë¸ ë¡œë”© ì½”ë“œë¥¼ ì—¬ê¸°ì— êµ¬í˜„
            # ì˜ˆ: GAN, Stable Diffusion, StyleGAN ë“±
            # self.model = torch.load(model_path, map_location=self.device)
            # self.model.eval()
            self.pipe = AutoPipelineForText2Image.from_pretrained(
            "stabilityai/sdxl-turbo",
            torch_dtype=torch.float16,  # fp16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì ˆë°˜ìœ¼ë¡œ ì¤„ì„
            variant="fp16",
            use_safetensors=True,  # ì•ˆì „í•œ í…ì„œ í˜•ì‹ ì‚¬ìš©
            low_cpu_mem_usage=True  # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
            )

            # GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ CUDAë¡œ ì´ë™
            if torch.cuda.is_available():
                self.pipe.to("cuda")
                # GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
                torch.backends.cudnn.benchmark = True  # cuDNN ìµœì í™”
                torch.backends.cuda.matmul.allow_tf32 = True  # TF32 ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
                logger.info(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
            else:
                logger.warning("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

            model_load_time = time.time() - model_load_start_time
            logger.info(f"ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ì†Œìš”ì‹œê°„: {model_load_time:.2f}ì´ˆ")

        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # print(f"ìºë¦­í„° ìƒì„± ëª¨ë¸ì´ {self.device}ì—ì„œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ëª¨ë¸ warmup ì‹¤í–‰ìœ¼ë¡œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ìƒì„± ì†ë„ ê°œì„ 
        self._warmup_model()

    def _warmup_model(self):
        """
        ëª¨ë¸ warmupì„ í†µí•œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ìƒì„± ì†ë„ ìµœì í™”
        
        ëª©ì :
        1. CUDA ì»¤ë„ ì´ˆê¸°í™”: GPUì—ì„œ ì²˜ìŒ ì—°ì‚° ì‹œ ë°œìƒí•˜ëŠ” ì§€ì—°ì„ ë¯¸ë¦¬ ì²˜ë¦¬
        2. ë©”ëª¨ë¦¬ í• ë‹¹: í•„ìš”í•œ GPU ë©”ëª¨ë¦¬ë¥¼ ë¯¸ë¦¬ í• ë‹¹í•˜ê³  ìºì‹±
        3. ê°€ì¤‘ì¹˜ ìµœì í™”: ëª¨ë¸ ê°€ì¤‘ì¹˜ì˜ GPU ìµœì í™”ë¥¼ ì‚¬ì „ì— ì™„ë£Œ
        4. cuDNN ì•Œê³ ë¦¬ì¦˜ ì„ íƒ: ìµœì ì˜ convolution ì•Œê³ ë¦¬ì¦˜ì„ ë¯¸ë¦¬ ê²°ì •
        
        ì´ ê³¼ì •ì„ í†µí•´ ì‹¤ì œ ì‚¬ìš©ì ìš”ì²­ ì‹œ ëŒ€ê¸°ì‹œê°„ì„ í¬ê²Œ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        logger.info("ğŸ”¥ ëª¨ë¸ warmup ì‹œì‘ - ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ìƒì„± ì†ë„ ìµœì í™”ë¥¼ ìœ„í•œ ì¤€ë¹„ ì‘ì—…")
        warmup_start_time = time.time()
        
        try:
            # warmupìš© ê°„ë‹¨í•œ ë”ë¯¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
            # ì‹¤ì œ ì‚¬ìš©ë  í”„ë¡¬í”„íŠ¸ì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ êµ¬ì„±í•˜ì—¬ ë™ì¼í•œ ì—°ì‚° ê²½ë¡œë¥¼ ê±°ì¹˜ë„ë¡ í•¨
            warmup_prompt = "cute butterfly, soft pastels, simple details, plain background"
            
            logger.info(f"ë”ë¯¸ í”„ë¡¬í”„íŠ¸ë¡œ warmup ì‹¤í–‰: '{warmup_prompt}'")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (warmup ì „)
            if torch.cuda.is_available():
                memory_before_warmup = torch.cuda.memory_allocated()
                logger.info(f"Warmup ì „ GPU ë©”ëª¨ë¦¬: {memory_before_warmup / 1024**3:.2f} GB")
            
            # ì‹¤ì œ ì¶”ë¡ ê³¼ ë™ì¼í•œ ì¡°ê±´ìœ¼ë¡œ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            # torch.no_grad()ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
            with torch.no_grad():
                # ì²« ë²ˆì§¸ ì¶”ë¡ ì—ì„œ ë°œìƒí•˜ëŠ” ëª¨ë“  ì´ˆê¸°í™” ì‘ì—…ì„ ì—¬ê¸°ì„œ ì²˜ë¦¬
                warmup_image = self.pipe(
                    prompt=warmup_prompt,       # ë”ë¯¸ í”„ë¡¬í”„íŠ¸
                    num_inference_steps=2,      # ë¹ ë¥¸ ìƒì„±ì„ ìœ„í•´ 1ìŠ¤í… (ì‹¤ì œ ì‚¬ìš©ê³¼ ë™ì¼)
                    guidance_scale=1.5,         # ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ 0 (ì‹¤ì œ ì‚¬ìš©ê³¼ ë™ì¼)
                    width=640,                  # ì‹¤ì œ ì‚¬ìš©ê³¼ ë™ì¼í•œ í•´ìƒë„
                    height=400                  # ì‹¤ì œ ì‚¬ìš©ê³¼ ë™ì¼í•œ í•´ìƒë„
                ).images[0]
            
            # warmupì—ì„œ ìƒì„±ëœ ì´ë¯¸ì§€ëŠ” ë©”ëª¨ë¦¬ì—ì„œ ì¦‰ì‹œ ì‚­ì œ
            # íŒŒì¼ë¡œ ì €ì¥í•˜ì§€ ì•Šê³  ë©”ëª¨ë¦¬ë§Œ ì‚¬ìš©í•˜ì—¬ ë””ìŠ¤í¬ I/O ìµœì†Œí™”
            del warmup_image
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (warmup í›„)
            if torch.cuda.is_available():
                memory_after_warmup = torch.cuda.memory_allocated()
                logger.info(f"Warmup í›„ GPU ë©”ëª¨ë¦¬: {memory_after_warmup / 1024**3:.2f} GB")
                memory_allocated = (memory_after_warmup - memory_before_warmup) / 1024**2
                logger.info(f"Warmup ë©”ëª¨ë¦¬ í• ë‹¹ëŸ‰: {memory_allocated:.2f} MB")
                
                # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ë¡œ ë¶ˆí•„ìš”í•œ ë©”ëª¨ë¦¬ í•´ì œ
                torch.cuda.empty_cache()
                logger.info("GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            
            # warmup ì™„ë£Œ ì‹œê°„ ì¸¡ì • ë° ë¡œê¹…
            warmup_time = time.time() - warmup_start_time
            logger.info(f"âœ… ëª¨ë¸ warmup ì™„ë£Œ! ì†Œìš”ì‹œê°„: {warmup_time:.2f}ì´ˆ")
            logger.info("ì´ì œ ì‚¬ìš©ì ìš”ì²­ ì‹œ ë¹ ë¥¸ ì´ë¯¸ì§€ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            
        except Exception as e:
            # warmup ì‹¤íŒ¨ ì‹œì—ë„ ì„œë¹„ìŠ¤ëŠ” ì •ìƒ ë™ì‘í•˜ë„ë¡ ì˜ˆì™¸ ì²˜ë¦¬
            # warmup ì‹¤íŒ¨ê°€ ì „ì²´ ì„œë¹„ìŠ¤ë¥¼ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•Šë„ë¡ ì£¼ì˜
            logger.warning(f"âš ï¸ ëª¨ë¸ warmup ì‹¤íŒ¨ (ì„œë¹„ìŠ¤ëŠ” ì •ìƒ ë™ì‘): {e}")
            logger.info("ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ìƒì„± ì‹œ ë‹¤ì†Œ ì§€ì—°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    
    async def generate(self, keyword: str):
        """
        ìºë¦­í„° ìƒì„± ë©”ì¸ í•¨ìˆ˜
        
        Args:
            image_path: ì›ë³¸ ê³¤ì¶© ì´ë¯¸ì§€ ê²½ë¡œ
            style: ìºë¦­í„° ìŠ¤íƒ€ì¼ (ì„ íƒì‚¬í•­)
            
        Returns:
            ìƒì„± ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """        
        # ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if self.pipe is None:
            raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        
        try:
            # ì „ì²´ ìš”ì²­ ì‹œì‘ ì‹œê°„
            total_start_time = time.time()
            print("1")
            # ìš”ì²­ ì‹œì‘ ë¡œê·¸
            logger.info(f"ì´ë¯¸ì§€ ìƒì„± ìš”ì²­: '{keyword}'")

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompt.format(keyword=keyword)
            logger.info(f"ìƒì„±ëœ í”„ë¡¬í”„íŠ¸: {prompt[:100]}...")

            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹… (ìš”ì²­ ì „)
            if torch.cuda.is_available():
                memory_before = torch.cuda.memory_allocated()
                logger.info(f"ìƒì„± ì „ GPU ë©”ëª¨ë¦¬: {memory_before / 1024**3:.2f} GB")

            # ì´ë¯¸ì§€ ìƒì„± ì‹œì‘ ì‹œê°„
            generation_start_time = time.time()

            # ì´ë¯¸ì§€ ìƒì„± (ìµœì í™”ëœ ì„¤ì •)
            with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
                image = self.pipe(
                    prompt=prompt,
                    # negative_prompt=self.negative_prompt,
                    num_inference_steps=3,  # ë¹ ë¥¸ ìƒì„±ì„ ìœ„í•´ 3ìŠ¤í… ì‚¬ìš©
                    guidance_scale=1.5,     # ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ 1.5ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë¹ ë¥¸ ìƒì„±
                    width=640,              # ì¹´ë“œ ë¹„ìœ¨ì— ë§ê²Œ ê°€ë¡œë¥¼ ë” ë„“ê²Œ (16:10 ë¹„ìœ¨)
                    height=400              # ì¹´ë“œ ì´ë¯¸ì§€ ì„¹ì…˜ì— ë§ëŠ” ë†’ì´
                ).images[0]

            # ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ ì‹œê°„
            generation_time = time.time() - generation_start_time

            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹… (ìš”ì²­ í›„)
            if torch.cuda.is_available():
                memory_after = torch.cuda.memory_allocated()
                logger.info(f"ìƒì„± í›„ GPU ë©”ëª¨ë¦¬: {memory_after / 1024**3:.2f} GB")
                logger.info(f"ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰: {(memory_after - memory_before) / 1024**2:.2f} MB")

            # ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹œì‘ ì‹œê°„
            encoding_start_time = time.time()

            # ì´ë¯¸ì§€ë¥¼ ë©”ëª¨ë¦¬ ë²„í¼ì— ì €ì¥
            buf = io.BytesIO()
            image.save(buf, format="PNG", optimize=True)  # PNG ìµœì í™” ì˜µì…˜ ì¶”ê°€
            buf.seek(0)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{timestamp}_{keyword}.png"

            # ìƒì„±ëœ ì´ë¯¸ì§€ íŒŒì¼ì— ì €ì¥
            output_dir = "out_put_image"
            os.makedirs(output_dir, exist_ok=True)
            file_path = os.path.join(output_dir, filename)

            with open(file_path, "wb") as f:
                f.write(buf.getbuffer())

            # ì´ë¯¸ì§€ ì¸ì½”ë”© ì™„ë£Œ ì‹œê°„
            encoding_time = time.time() - encoding_start_time

            # ìƒì„±ëœ ì´ë¯¸ì§€ ê°ì²´ ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
            del image

            # ì „ì²´ ìš”ì²­ ì™„ë£Œ ì‹œê°„
            total_time = time.time() - total_start_time

            # ì„±ëŠ¥ í†µê³„ ë¡œê¹…
            logger.info(f"ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: '{keyword}'")
            logger.info(f"ğŸ“Š ì„±ëŠ¥ í†µê³„:")
            logger.info(f"   - ì „ì²´ ì†Œìš”ì‹œê°„: {total_time:.3f}ì´ˆ")
            logger.info(f"   - ì´ë¯¸ì§€ ìƒì„±: {generation_time:.3f}ì´ˆ")
            logger.info(f"   - ì´ë¯¸ì§€ ì¸ì½”ë”©: {encoding_time:.3f}ì´ˆ")

            if torch.cuda.is_available():
                memory_used = (memory_after - memory_before) / 1024**2
                logger.info(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f} MB")

            # PNG ì´ë¯¸ì§€ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µìœ¼ë¡œ ë°˜í™˜ (ì„±ëŠ¥ ì •ë³´ í—¤ë” í¬í•¨)
            return filename

        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
            # cleanup_memory()
            raise HTTPException(status_code=500, detail=f"ì´ë¯¸ì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
